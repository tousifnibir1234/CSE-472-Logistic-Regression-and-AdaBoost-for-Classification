{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e6458a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(9999)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ba768",
   "metadata": {},
   "source": [
    "## Preprocessing of TElco Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "30948173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5634 31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\",na_values=[' ','?'])\n",
    "rowNum=df.shape[0]\n",
    "\n",
    "\n",
    "df.drop('customerID',inplace= True, axis=1)\n",
    "df['TotalCharges'].fillna(value=df['TotalCharges'].mean(),inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "##One hot encoding\n",
    "df = pd.get_dummies(df,drop_first=True) \n",
    "\n",
    "#standardize\n",
    "ss=StandardScaler()\n",
    "df.iloc[:,:]=ss.fit_transform(df.iloc[:,:])\n",
    "# df=(df-df.min())/(df.max()-df.min())\n",
    "\n",
    "dummyCol=np.ones(rowNum)#add a  1 column  to dataframe beginning\n",
    "df.insert(loc=0, column='dummy', value=dummyCol)\n",
    "\n",
    "dataset=df.copy()\n",
    "\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=99)\n",
    "df=train\n",
    "\n",
    "Y_out = df.iloc[:, -1]  #i final output\n",
    "Y_out = np.sign(Y_out)  #convert to -1 and +1\n",
    "df.drop(df.columns[-1], axis=1,inplace= True)  #drop the last column\n",
    "\n",
    "# Dummy column full of ones at beginning of a matrix\n",
    "rowNum=df.shape[0]\n",
    "colNum= df.shape[1]\n",
    "print(rowNum, colNum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc931a",
   "metadata": {},
   "source": [
    "# Dataset 2 preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b76cf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26048 98\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dummy</th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>workclass_ Never-worked</th>\n",
       "      <th>workclass_ Private</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_ Portugal</th>\n",
       "      <th>native-country_ Puerto-Rico</th>\n",
       "      <th>native-country_ Scotland</th>\n",
       "      <th>native-country_ South</th>\n",
       "      <th>native-country_ Taiwan</th>\n",
       "      <th>native-country_ Thailand</th>\n",
       "      <th>native-country_ Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_ United-States</th>\n",
       "      <th>native-country_ Vietnam</th>\n",
       "      <th>native-country_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32297</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.397228</td>\n",
       "      <td>-0.282542</td>\n",
       "      <td>-0.031325</td>\n",
       "      <td>-0.145914</td>\n",
       "      <td>-0.216663</td>\n",
       "      <td>-0.03543</td>\n",
       "      <td>-0.262102</td>\n",
       "      <td>-0.014664</td>\n",
       "      <td>0.572054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033729</td>\n",
       "      <td>-0.059275</td>\n",
       "      <td>-0.019201</td>\n",
       "      <td>-0.049629</td>\n",
       "      <td>-0.039608</td>\n",
       "      <td>-0.023519</td>\n",
       "      <td>-0.024164</td>\n",
       "      <td>0.307214</td>\n",
       "      <td>-0.045409</td>\n",
       "      <td>-0.022173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24643</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.335886</td>\n",
       "      <td>0.742270</td>\n",
       "      <td>-0.808728</td>\n",
       "      <td>-0.145914</td>\n",
       "      <td>-0.216663</td>\n",
       "      <td>-0.03543</td>\n",
       "      <td>-0.262102</td>\n",
       "      <td>-0.014664</td>\n",
       "      <td>0.572054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033729</td>\n",
       "      <td>-0.059275</td>\n",
       "      <td>-0.019201</td>\n",
       "      <td>-0.049629</td>\n",
       "      <td>-0.039608</td>\n",
       "      <td>-0.023519</td>\n",
       "      <td>-0.024164</td>\n",
       "      <td>0.307214</td>\n",
       "      <td>-0.045409</td>\n",
       "      <td>-0.022173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17622</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.323917</td>\n",
       "      <td>-0.057005</td>\n",
       "      <td>-0.031325</td>\n",
       "      <td>-0.145914</td>\n",
       "      <td>-0.216663</td>\n",
       "      <td>-0.03543</td>\n",
       "      <td>-0.262102</td>\n",
       "      <td>-0.014664</td>\n",
       "      <td>0.572054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033729</td>\n",
       "      <td>-0.059275</td>\n",
       "      <td>-0.019201</td>\n",
       "      <td>-0.049629</td>\n",
       "      <td>-0.039608</td>\n",
       "      <td>-0.023519</td>\n",
       "      <td>-0.024164</td>\n",
       "      <td>0.307214</td>\n",
       "      <td>-0.045409</td>\n",
       "      <td>-0.022173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32030</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.262575</td>\n",
       "      <td>0.054328</td>\n",
       "      <td>-2.363533</td>\n",
       "      <td>-0.145914</td>\n",
       "      <td>-0.216663</td>\n",
       "      <td>-0.03543</td>\n",
       "      <td>-0.262102</td>\n",
       "      <td>-0.014664</td>\n",
       "      <td>0.572054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033729</td>\n",
       "      <td>-0.059275</td>\n",
       "      <td>-0.019201</td>\n",
       "      <td>-0.049629</td>\n",
       "      <td>-0.039608</td>\n",
       "      <td>-0.023519</td>\n",
       "      <td>-0.024164</td>\n",
       "      <td>-3.255064</td>\n",
       "      <td>-0.045409</td>\n",
       "      <td>-0.022173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16784</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.397228</td>\n",
       "      <td>-0.308530</td>\n",
       "      <td>0.746077</td>\n",
       "      <td>-0.145914</td>\n",
       "      <td>-0.216663</td>\n",
       "      <td>-0.03543</td>\n",
       "      <td>-0.262102</td>\n",
       "      <td>-0.014664</td>\n",
       "      <td>0.572054</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033729</td>\n",
       "      <td>-0.059275</td>\n",
       "      <td>-0.019201</td>\n",
       "      <td>-0.049629</td>\n",
       "      <td>-0.039608</td>\n",
       "      <td>-0.023519</td>\n",
       "      <td>-0.024164</td>\n",
       "      <td>0.307214</td>\n",
       "      <td>-0.045409</td>\n",
       "      <td>-0.022173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dummy       age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "32297    1.0  0.397228 -0.282542      -0.031325     -0.145914     -0.216663   \n",
       "24643    1.0 -0.335886  0.742270      -0.808728     -0.145914     -0.216663   \n",
       "17622    1.0  0.323917 -0.057005      -0.031325     -0.145914     -0.216663   \n",
       "32030    1.0 -0.262575  0.054328      -2.363533     -0.145914     -0.216663   \n",
       "16784    1.0  0.397228 -0.308530       0.746077     -0.145914     -0.216663   \n",
       "\n",
       "       hours-per-week  workclass_ Local-gov  workclass_ Never-worked  \\\n",
       "32297        -0.03543             -0.262102                -0.014664   \n",
       "24643        -0.03543             -0.262102                -0.014664   \n",
       "17622        -0.03543             -0.262102                -0.014664   \n",
       "32030        -0.03543             -0.262102                -0.014664   \n",
       "16784        -0.03543             -0.262102                -0.014664   \n",
       "\n",
       "       workclass_ Private  ...  native-country_ Portugal  \\\n",
       "32297            0.572054  ...                 -0.033729   \n",
       "24643            0.572054  ...                 -0.033729   \n",
       "17622            0.572054  ...                 -0.033729   \n",
       "32030            0.572054  ...                 -0.033729   \n",
       "16784            0.572054  ...                 -0.033729   \n",
       "\n",
       "       native-country_ Puerto-Rico  native-country_ Scotland  \\\n",
       "32297                    -0.059275                 -0.019201   \n",
       "24643                    -0.059275                 -0.019201   \n",
       "17622                    -0.059275                 -0.019201   \n",
       "32030                    -0.059275                 -0.019201   \n",
       "16784                    -0.059275                 -0.019201   \n",
       "\n",
       "       native-country_ South  native-country_ Taiwan  \\\n",
       "32297              -0.049629               -0.039608   \n",
       "24643              -0.049629               -0.039608   \n",
       "17622              -0.049629               -0.039608   \n",
       "32030              -0.049629               -0.039608   \n",
       "16784              -0.049629               -0.039608   \n",
       "\n",
       "       native-country_ Thailand  native-country_ Trinadad&Tobago  \\\n",
       "32297                 -0.023519                        -0.024164   \n",
       "24643                 -0.023519                        -0.024164   \n",
       "17622                 -0.023519                        -0.024164   \n",
       "32030                 -0.023519                        -0.024164   \n",
       "16784                 -0.023519                        -0.024164   \n",
       "\n",
       "       native-country_ United-States  native-country_ Vietnam  \\\n",
       "32297                       0.307214                -0.045409   \n",
       "24643                       0.307214                -0.045409   \n",
       "17622                       0.307214                -0.045409   \n",
       "32030                      -3.255064                -0.045409   \n",
       "16784                       0.307214                -0.045409   \n",
       "\n",
       "       native-country_ Yugoslavia  \n",
       "32297                   -0.022173  \n",
       "24643                   -0.022173  \n",
       "17622                   -0.022173  \n",
       "32030                   -0.022173  \n",
       "16784                   -0.022173  \n",
       "\n",
       "[5 rows x 98 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"adult.data\",na_values = [\" \",' ?',np.NaN])\n",
    "rowNum=df.shape[0]\n",
    "\n",
    "df.columns=[\"age\",\"workclass\",\"fnlwgt\",\"education\",\"education-num\",\"marital-status\",\"occupation\",\"relationship\",\"race\",\n",
    "                  \"sex\",\"capital-gain\",\"capital-loss\",\"hours-per-week\",\"native-country\",\"income\"]\n",
    "obj=[]\n",
    "\n",
    "##StackOverflow\n",
    "catCols=df.select_dtypes(include=['object']).columns.tolist()\n",
    "for column in df:\n",
    "    if df[column].isnull().any():\n",
    "        if(column in catCols):\n",
    "            df[column]=df[column].fillna(df[column].mode()[0])\n",
    "        else:\n",
    "            df[column]=df[column].fillna(df[column].mean)\n",
    "    \n",
    "df = pd.get_dummies(df,drop_first=True) \n",
    "#standardize\n",
    "ss=StandardScaler()\n",
    "df.iloc[:,:]=ss.fit_transform(df.iloc[:,:])\n",
    "# df=(df-df.min())/(df.max()-df.min())\n",
    "\n",
    "dummyCol=np.ones(rowNum)#add a  1 column  to dataframe beginning\n",
    "df.insert(loc=0, column='dummy', value=dummyCol)\n",
    "\n",
    "dataset=df.copy()\n",
    "\n",
    "\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=99)\n",
    "df=train\n",
    "\n",
    "Y_out = df.iloc[:, -1]  #i final output\n",
    "Y_out = np.sign(Y_out)  #convert to -1 and +1\n",
    "df.drop(df.columns[-1], axis=1,inplace= True)  #drop the last column\n",
    "\n",
    "# Dummy column full of ones at beginning of a matrix\n",
    "rowNum=df.shape[0]\n",
    "colNum= df.shape[1]\n",
    "print(rowNum, colNum)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6f1a94f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ...  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_out.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac0886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "\n",
    "df.iloc[0]['SeniorCitizen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84ab8c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4936983435430413\n",
      "14256   5537   857   5398\n",
      "accuracy is  0.7545300982800983\n",
      "sensitivity is  0.8629896083133494\n",
      "specificity is  0.7202546354771889\n",
      "precision  0.4936442615454961\n",
      "fdr   0.5063557384545039\n",
      "f1 score  0.6280395578824898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.98522451e+01,  1.79767214e+01, -2.14425357e+00,  1.76519685e+01,\n",
       "        1.23454228e+01,  7.80996724e+00,  1.03746813e+01, -3.08612460e+00,\n",
       "        9.42182709e-01,  4.42489098e+00,  3.13352470e+00, -6.21722058e+00,\n",
       "        1.07061025e+00,  1.05166345e+00, -6.68709701e+00, -7.31353637e+00,\n",
       "       -2.66762234e+00, -5.88336456e-01, -2.34390242e+00, -4.17528261e+00,\n",
       "        1.20768550e+00,  3.14252018e+00,  1.08867953e+01,  3.62510058e+00,\n",
       "       -8.43947485e+00,  1.15696486e+01,  8.53522459e-01,  4.75454931e+00,\n",
       "       -5.64562427e-02,  1.47183630e+00,  3.09003740e+01,  1.22734298e+00,\n",
       "       -2.52788280e+01, -9.10759268e+00,  1.56826415e-02,  1.19424861e+00,\n",
       "       -1.06881691e+01,  1.51834423e+01,  4.16064719e+00, -6.56920826e+00,\n",
       "        1.03029331e+00, -5.01468150e+00,  2.47906791e+00,  8.31417038e+00,\n",
       "        2.48568294e+00, -1.15411979e+01,  2.73372041e+00, -2.84342723e-01,\n",
       "       -1.16990411e+01,  3.45906657e+00, -1.77870354e+01, -1.21996403e+01,\n",
       "        6.37214567e+00,  2.27534867e+00, -8.39840928e+00,  2.79338211e+00,\n",
       "        6.94711256e+00,  1.18276544e+01,  2.26033532e+00,  2.41625917e+00,\n",
       "       -1.08405855e+00,  9.80507610e-01,  2.06657007e+00,  7.14925894e-01,\n",
       "        2.05377825e+00,  2.07491087e+00,  1.38359912e+00,  1.71198550e+00,\n",
       "        8.54101992e-01,  1.51685038e+00, -3.07899158e-02,  4.12204810e-01,\n",
       "        9.26810808e-01,  3.01767533e-01,  1.17060542e+00,  1.66091986e+00,\n",
       "        2.93912944e-01,  1.11073578e+00,  1.59930603e+00,  1.68293267e+00,\n",
       "        1.83320919e+00,  1.19197615e+00, -5.65995497e+00,  1.34806125e+00,\n",
       "        9.52766012e-01,  1.30778481e+00,  4.08116786e-02,  1.99544135e+00,\n",
       "       -7.59332026e-01,  1.82211463e+00,  1.05559087e+00,  7.40348868e-01,\n",
       "        1.56269618e+00,  1.00485062e+00,  9.62233082e-01, -2.41118710e-01,\n",
       "        1.61314607e+00,  1.41566959e+00])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def logisticRegression(data,result):\n",
    "    checker=100000000\n",
    "    oneMatrix= np.ones(rowNum)\n",
    "#     weight=np.random.rand(colNum)\n",
    "    weight= np.zeros(colNum)\n",
    "    counter=0\n",
    "    while (checker >0.5  and counter<50000 ):\n",
    "        counter= counter +1\n",
    "        if(counter %10000 ==0) :\n",
    "            print(counter)\n",
    "        alpha = 0.01\n",
    "        tempDf= data.to_numpy()\n",
    "        Yp = np.tanh(tempDf.dot(weight))\n",
    "    #         Y = (df.iloc[:,-1]).to_numpy()\n",
    "        Y= result\n",
    "        M1 =1 - Yp **2\n",
    "#         print(M1)\n",
    "        M2= np.multiply( Y- Yp,M1)\n",
    "        M2 = alpha * M2\n",
    "        \n",
    "        transposedDf= tempDf.transpose()\n",
    "        r=transposedDf.dot(M2)\n",
    "#         weight= weight+ np.multiply(r,alpha)\n",
    "        weight = weight +r\n",
    "        checker = np.sum((Y-Yp)**2)/(2* rowNum)\n",
    "    print(checker)\n",
    "#     print(weight)\n",
    "\n",
    "    predicted= np.sign(np.tanh(tempDf.dot(weight)))\n",
    "    confusionFunctions(result,predicted)\n",
    "\n",
    "#     acc= accuracy_score(Y_out, predicted)\n",
    "#     print(\"accuracy value \" ,acc)\n",
    "#     print(weight)\n",
    "    return weight\n",
    "\n",
    "\n",
    "logisticRegression(df,Y_out)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563989e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.sign(Y_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7985ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f390f51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def  WeightedMajority(h, z,data):\n",
    "    h= np.array(h)\n",
    "    z= np.array(z)\n",
    "    print('z is ', z)\n",
    "    print('h shape is ', h.shape)\n",
    "    data= data.transpose()\n",
    "    print(' data shape is ', data.shape)\n",
    "    M1= h@data\n",
    "    print(' m1 shape is ', M1.shape)\n",
    "    p=np.tanh(M1)\n",
    "    print(' p  is ',p)\n",
    "    print(' P shape is ', p.shape)\n",
    "    print ( '  Z shape is ',z.shape)\n",
    "    predict=z@p\n",
    "\n",
    "    print(' predict shape is ', predict.shape)\n",
    "    predict = np.sign(predict)\n",
    "    return predict\n",
    "\n",
    "# def  WeightedMajority(h, z,data):\n",
    "#     print('z is ',z)\n",
    "#     predict = np.zeros(len(data))\n",
    "#     for i in range(len(h)):\n",
    "#         predict= predict + z[i] * np.tanh(data.dot(h[i]))\n",
    "#     print('predict is', predict)\n",
    "#     predict= np.sign(predict)\n",
    "#     for i in predict: \n",
    "#         if ( i > 0):\n",
    "#             print('ok')\n",
    "#     print(predict)\n",
    "    \n",
    "def confusionFunctions (  yTrue ,yPredict):\n",
    "    tn, fp, fn, tp = confusion_matrix(yTrue,yPredict).ravel()\n",
    "    print(tn, \" \", fp, \" \",fn , \" \",tp)\n",
    "    accuracy = (tp+ tn)/(tp+fp+fn+tn)\n",
    "    print('accuracy is ',accuracy)\n",
    "    \n",
    "    \n",
    "    sensitivity = tp/(tp+fn)\n",
    "    print('sensitivity is ',sensitivity)\n",
    "    \n",
    "    specificity = tn/(tn+fp)\n",
    "    print('specificity is ',specificity)\n",
    "    \n",
    "    precision = tp/(tp+fp)\n",
    "    print('precision ',precision)\n",
    "    \n",
    "    fdr = fp/(fp+tp)\n",
    "    print('fdr  ',fdr)\n",
    "    \n",
    "    f1_score=  2*tp/( 2*tp +fp + fn)\n",
    "    print('f1 score ',f1_score)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def adaboost(iteration):\n",
    "# print(df.iloc[0])\n",
    "    w=np.empty(rowNum)\n",
    "    w.fill(1/rowNum)\n",
    "    h=[]\n",
    "    z=[]\n",
    "    for k in range(iteration):\n",
    "        df['ans'] = Y_out\n",
    "\n",
    "        data =df.sample(n= len(df),replace= True, weights=w,random_state=97)\n",
    "        result = data['ans'].to_numpy()\n",
    "        \n",
    "        df.drop('ans',inplace= True, axis=1)\n",
    "        data.drop('ans',inplace= True, axis=1)\n",
    "        wL=logisticRegression(data,result)\n",
    "        predicted= np.sign(np.tanh((data.to_numpy()).dot(wL)))\n",
    "        \n",
    "        \n",
    "        error =0 \n",
    "        for j in range(len(data)):\n",
    "            if ( predicted[j] != result[j]) :\n",
    "                error = error + w[j]\n",
    "        if (error> 0.5 ):\n",
    "            continue \n",
    "        h.append(wL)\n",
    "\n",
    "\n",
    "        for j in range(len(data)):\n",
    "            if( predicted[j] == result[j]):\n",
    "                w[j]=w[j] *(error)/(1-error)\n",
    "                \n",
    "        # Normalize W \n",
    "        s=np.sum(w)\n",
    "        w= w/s\n",
    "        # Finding z\n",
    "        q=math.log2((1-error)/error)\n",
    "#         print('q is ',q )\n",
    "        z.append(q)\n",
    "    predict= WeightedMajority(h,z,df)\n",
    "    confusionFunctions(Y_out,predict)\n",
    "\n",
    "        \n",
    "adaboost(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12575617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
